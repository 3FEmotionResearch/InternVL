{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b8fff0-5d64-4eea-a554-e373873742e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/25-07-11-internvl-2/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/paperspace/miniconda3/envs/25-07-11-internvl-2/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù) utils.\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n",
    "# If you set `load_in_8bit=True`, you will need two 80GB GPUs.\n",
    "# If you set `load_in_8bit=False`, you will need at least three 80GB GPUs.\n",
    "path = 'OpenGVLab/InternVL3-1B'\n",
    "device_map = split_model('InternVL3-1B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True, temperature=0.7, top_p=0.95, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e17be9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: ‰Ω†Â•ΩÂêó\n",
      "Assistant: ‰Ω†Â•ΩÔºÅÁúãËµ∑Êù•ÊàëÂèØËÉΩÊúâÁÇπÂøôÁ¢åÔºåÊâÄ‰ª•Ê≤°ÊúâÂÆûÊó∂ÁöÑÂç≥Êó∂Ê∂àÊÅØÊàñÂõûÂ∫î„ÄÇËØ∑ÈöèÊó∂ÂëäËØâÊàë‰Ω†ÈúÄË¶Å‰ªÄ‰πàÂ∏ÆÂä©Êàñ‰ø°ÊÅØÔºåÊàë‰ºöÂ∞ΩÂäõ‰∏∫‰Ω†ÊúçÂä°ÔºÅ\n"
     ]
    }
   ],
   "source": [
    "# pure-text conversation (Á∫ØÊñáÊú¨Â§öËΩÆÂØπËØù)\n",
    "question = '‰Ω†Â•ΩÂêó'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e7fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image features a red panda with distinctive brown and white fur, resting on a piece of wood in a natural setting with green foliage in the background. The red panda appears relaxed and is looking directly at the camera.\n"
     ]
    }
   ],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "\n",
    "# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085d37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Frame1: <image>\n",
      "Frame2: <image>\n",
      "Frame3: <image>\n",
      "Frame4: <image>\n",
      "Frame5: <image>\n",
      "Frame6: <image>\n",
      "Frame7: <image>\n",
      "Frame8: <image>\n",
      "Frame9: <image>\n",
      "Frame10: <image>\n",
      "<subtitle>\n",
      "ÈÇ£Â∞±‰∏çËßÅ‰∏çÊï£‰∫ÜÂïäÔºåÂìéÔºåÂ•ΩÔºåÂÜçËßÅ\n",
      "</subtitle>\n",
      "Please identify the emotional states of the main character with specific reasons.\n",
      "Assistant: The woman appears to be in a cheerful and happy mood. She smiles while holding the phone to her ear, indicating a sense of relief or satisfaction with the conversation. Her facial expressions suggest she is enjoying the conversation, further emphasizing the positive emotion she is conveying.\n"
     ]
    }
   ],
   "source": [
    "subtitle_text = 'ÈÇ£Â∞±‰∏çËßÅ‰∏çÊï£‰∫ÜÂïäÔºåÂìéÔºåÂ•ΩÔºåÂÜçËßÅ'\n",
    "\n",
    "video_path = '/home/paperspace/Downloads/25-07-11-affectgpt-dataset-mini100/video/sample_00000007.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=10, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + '<subtitle>\\n' + subtitle_text + '\\n</subtitle>\\n' + 'Please identify the emotional states of the main character with specific reasons.'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4574bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Please extract emotion words into a list. Please separate different emotional categories with commas and output only the clearly identifiable emotional categories in a list format. If none are identified, please output an empty list. The output words have to be adjectives of feelings from those emotion words (internal state), not an action/expression (external manifestation).\n",
      "\n",
      "EXAMPLES:\n",
      "‚úì Good: [\"happy\", \"excited\", \"nervous\"]\n",
      "‚úó Bad: [\"smiling\", \"jumping\", \"talking loudly\"]\n",
      "\n",
      "Assistant: , [\"happy\", \"relieved\", \"smiling\", \"joyful\"]\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Please extract emotion words into a list. Please separate different emotional categories with commas and output only the clearly identifiable emotional categories in a list format. If none are identified, please output an empty list. The output words have to be adjectives of feelings from those emotion words (internal state), not an action/expression (external manifestation).\n",
    "\n",
    "EXAMPLES:\n",
    "‚úì Good: [\"happy\", \"excited\", \"nervous\"]\n",
    "‚úó Bad: [\"smiling\", \"jumping\", \"talking loudly\"]\n",
    "\"\"\"\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a559e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_repo_files\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d534c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_merged_dataset(local_path=None):\n",
    "    \"\"\"\n",
    "    Create enhanced merged dataset with emotions, reasoning, and subtitle data.\n",
    "    \n",
    "    Args:\n",
    "        local_path (str, optional): Path to local folder containing CSV files.\n",
    "                                   If provided and all required files exist, \n",
    "                                   will read from local files instead of Hugging Face.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - enhanced_merged_by_name: Complete dataset with all merged data\n",
    "            - merged_by_name: Basic merged dataset (track2 + track3)\n",
    "            - subtitle_by_name: Subtitle data indexed by name\n",
    "            - datasets: All loaded individual datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Required CSV files\n",
    "    required_files = [\n",
    "        \"subtitle_chieng.csv\",\n",
    "        \"track2_train_mercaptionplus.csv\", \n",
    "        \"track2_train_ovmerd.csv\",\n",
    "        \"track3_train_mercaptionplus.csv\",\n",
    "        \"track3_train_ovmerd.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Check if we should use local files\n",
    "    use_local = False\n",
    "    if local_path and os.path.exists(local_path):\n",
    "        # Check if all required files exist in the local path\n",
    "        local_files_exist = all(\n",
    "            os.path.exists(os.path.join(local_path, file)) \n",
    "            for file in required_files\n",
    "        )\n",
    "        if local_files_exist:\n",
    "            use_local = True\n",
    "            csv_files = required_files\n",
    "            print(f\"‚úì Using local files from: {local_path}\")\n",
    "        else:\n",
    "            missing_files = [\n",
    "                file for file in required_files \n",
    "                if not os.path.exists(os.path.join(local_path, file))\n",
    "            ]\n",
    "            print(f\"‚ùå Missing files in {local_path}: {missing_files}\")\n",
    "            print(\"Falling back to Hugging Face...\")\n",
    "    \n",
    "    # If not using local files, discover files using Hugging Face Hub API\n",
    "    if not use_local:\n",
    "        try:\n",
    "            # List all files in the dataset repository\n",
    "            files = list_repo_files(\"MERChallenge/MER2025\", repo_type=\"dataset\")\n",
    "            \n",
    "            # Filter for CSV files\n",
    "            csv_files = [f for f in files if f.endswith('.csv')]\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to the required files list\n",
    "            csv_files = required_files\n",
    "\n",
    "    datasets = {}\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            dataset_name = csv_file.replace('.csv', '')\n",
    "            \n",
    "            if use_local:\n",
    "                # Load from local file\n",
    "                file_path = os.path.join(local_path, csv_file)\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                \n",
    "                # Convert to datasets format\n",
    "                from datasets import Dataset\n",
    "                dataset_dict = {\"train\": Dataset.from_pandas(df)}\n",
    "                datasets[dataset_name] = dataset_dict\n",
    "                \n",
    "            else:\n",
    "                # Load from Hugging Face\n",
    "                if csv_file == \"subtitle_chieng.csv\":\n",
    "                    # Special handling for subtitle_chieng.csv which has encoding issues with datasets library\n",
    "                    from huggingface_hub import hf_hub_download\n",
    "                    \n",
    "                    # Download the file first\n",
    "                    file_path = hf_hub_download(\n",
    "                        repo_id=\"MERChallenge/MER2025\",\n",
    "                        filename=csv_file,\n",
    "                        repo_type=\"dataset\"\n",
    "                    )\n",
    "                    \n",
    "                    # Load with pandas\n",
    "                    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                    \n",
    "                    # Convert to datasets format\n",
    "                    from datasets import Dataset\n",
    "                    dataset_dict = {\"train\": Dataset.from_pandas(df)}\n",
    "                    datasets[dataset_name] = dataset_dict\n",
    "                    \n",
    "                else:\n",
    "                    # Load without specifying split to see all available splits\n",
    "                    dataset_dict = load_dataset(\n",
    "                        \"MERChallenge/MER2025\", \n",
    "                        data_files=csv_file\n",
    "                    )\n",
    "                    datasets[dataset_name] = dataset_dict\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass  # Silently skip failed loads\n",
    "\n",
    "    merged_by_name = {}\n",
    "    \n",
    "    # Check if we have both datasets\n",
    "    if ('track2_train_mercaptionplus' in datasets and \n",
    "        'track3_train_mercaptionplus' in datasets):\n",
    "        \n",
    "        # Get the train splits from both datasets\n",
    "        track2_data = datasets['track2_train_mercaptionplus']['train']\n",
    "        track3_data = datasets['track3_train_mercaptionplus']['train']\n",
    "        \n",
    "        # Convert to pandas DataFrames for easier merging\n",
    "        track2_df = track2_data.to_pandas()\n",
    "        track3_df = track3_data.to_pandas()\n",
    "        \n",
    "        # Check if both have 'name' column\n",
    "        if 'name' in track2_df.columns and 'name' in track3_df.columns:\n",
    "            # Merge on 'name' column\n",
    "            merged_df = pd.merge(\n",
    "                track2_df, \n",
    "                track3_df, \n",
    "                on='name', \n",
    "                how='inner',  # Only keep rows that exist in both datasets\n",
    "                suffixes=('_track2', '_track3')\n",
    "            )\n",
    "            \n",
    "            # Create a list of dictionaries with combined data\n",
    "            merged_data = []\n",
    "            for _, row in merged_df.iterrows():\n",
    "                combined_row = {\n",
    "                    'name': row['name'],\n",
    "                    'openset': row.get('openset', None),  # from track2\n",
    "                    'reason': row.get('reason', None)     # from track3\n",
    "                }\n",
    "                \n",
    "                # Add all other columns from both datasets\n",
    "                for col in merged_df.columns:\n",
    "                    if col not in ['name', 'openset', 'reason']:\n",
    "                        combined_row[col] = row[col]\n",
    "                \n",
    "                merged_data.append(combined_row)\n",
    "            \n",
    "            # Create a dictionary indexed by name for easy access\n",
    "            merged_by_name = {}\n",
    "            for entry in merged_data:\n",
    "                name = entry['name']\n",
    "                merged_by_name[name] = entry\n",
    "\n",
    "    subtitle_by_name = {}\n",
    "    enhanced_merged_by_name = {}\n",
    "    \n",
    "    if 'subtitle_chieng' in datasets:\n",
    "        subtitle_data = datasets['subtitle_chieng']['train']\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        subtitle_df = subtitle_data.to_pandas()\n",
    "        \n",
    "        # Create subtitle dictionary indexed by name\n",
    "        subtitle_by_name = {}\n",
    "        for _, row in subtitle_df.iterrows():\n",
    "            name = row['name']\n",
    "            subtitle_by_name[name] = {\n",
    "                'name': name,\n",
    "                'chinese': row['chinese'] if pd.notna(row['chinese']) else None,\n",
    "                'english': row['english'] if pd.notna(row['english']) else None\n",
    "            }\n",
    "        \n",
    "        # Try to merge subtitle data with existing merged data if available\n",
    "        if merged_by_name:\n",
    "            # Add subtitle data to existing merged data\n",
    "            enhanced_merged_by_name = {}\n",
    "            for name, data in merged_by_name.items():\n",
    "                enhanced_data = data.copy()\n",
    "                \n",
    "                # Add subtitle information if available\n",
    "                if name in subtitle_by_name:\n",
    "                    subtitle_info = subtitle_by_name[name]\n",
    "                    enhanced_data.update({\n",
    "                        'chinese_subtitle': subtitle_info['chinese'],\n",
    "                        'english_subtitle': subtitle_info['english']\n",
    "                    })\n",
    "                else:\n",
    "                    enhanced_data.update({\n",
    "                        'chinese_subtitle': None,\n",
    "                        'english_subtitle': None\n",
    "                    })\n",
    "                \n",
    "                enhanced_merged_by_name[name] = enhanced_data\n",
    "            \n",
    "            # Show sample of enhanced data\n",
    "            print(f\"Sample of enhanced data with subtitles:\")\n",
    "            sample_names_with_subtitles = [name for name in list(enhanced_merged_by_name.keys())[:5] \n",
    "                                         if enhanced_merged_by_name[name]['chinese_subtitle'] or enhanced_merged_by_name[name]['english_subtitle']]\n",
    "            \n",
    "            for name in sample_names_with_subtitles[:2]:\n",
    "                data = enhanced_merged_by_name[name]\n",
    "                print(f\"  Name: {name}\")\n",
    "                print(f\"    openset: {data.get('openset', 'N/A')}\")\n",
    "                print(f\"    reason: {data.get('reason', 'N/A')[:100]}...\" if data.get('reason') and len(data.get('reason', '')) > 100 else f\"    reason: {data.get('reason', 'N/A')}\")\n",
    "                print(f\"    chinese_subtitle: {data.get('chinese_subtitle', 'N/A')}\")\n",
    "                print(f\"    english_subtitle: {data.get('english_subtitle', 'N/A')}\")\n",
    "                print()\n",
    "            \n",
    "    else:\n",
    "        # If no subtitle data, use merged data as enhanced data\n",
    "        if merged_by_name:\n",
    "            enhanced_merged_by_name = merged_by_name.copy()\n",
    "    \n",
    "    # Return all the created data structures\n",
    "    return enhanced_merged_by_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6298dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using local files from: /home/paperspace/Downloads/25-07-11-affectgpt-dataset-mini100\n",
      "Sample of enhanced data with subtitles:\n",
      "  Name: samplenew3_00000058\n",
      "    openset: [concern, pessimism]\n",
      "    reason: In the text, the subtitle reads: \"Given his precarious situation.\" Based on the description of the i...\n",
      "    chinese_subtitle: Âá≠‰ªñÁöÑÂ§ÑÂ¢É‰∏çÂ¶ô\n",
      "    english_subtitle: Given his situation is not good.\n",
      "\n",
      "  Name: samplenew3_00000084\n",
      "    openset: [concern, advice, care, dissatisfaction, worry]\n",
      "    reason: In the text, the subtitle content says, \"Please stop drinking.\" This sentence is likely spoken by th...\n",
      "    chinese_subtitle: Âà´Âñù‰∫ÜÂ•ΩÂêó\n",
      "    english_subtitle: Please don't drink anymore.\n",
      "\n",
      "{'name': 'samplenew3_00000058', 'openset': '[concern, pessimism]', 'reason': 'In the text, the subtitle reads: \"Given his precarious situation.\" Based on the description of the individual\\'s trembling voice and tone indicating feeling overwhelmed or under a lot of pressure in the audio clues, as well as the calm and composed facial expression, focused gaze, and body language of the woman in the video clues, we can infer that this sentence may be the woman\\'s evaluation or reaction to the man\\'s situation. The woman\\'s calm and focused demeanor suggests that she remains calm during the conversation and may have an understanding of the man\\'s predicament. Therefore, this sentence may indicate the woman\\'s concern or pessimistic view of the man\\'s situation, contrasting with the overall positive emotions displayed by the woman.', 'chinese_subtitle': 'Âá≠‰ªñÁöÑÂ§ÑÂ¢É‰∏çÂ¶ô', 'english_subtitle': 'Given his situation is not good.'}\n"
     ]
    }
   ],
   "source": [
    "enhanced_merged_by_name = create_enhanced_merged_dataset(local_path=\"/home/paperspace/Downloads/25-07-11-affectgpt-dataset-mini100\")\n",
    "\n",
    "print(enhanced_merged_by_name[list(enhanced_merged_by_name.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71d1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Processing: samplenew3_00000058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000058\n",
      "\n",
      "[2] Processing: samplenew3_00000084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000084\n",
      "\n",
      "[3] Processing: samplenew3_00000139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000139\n",
      "\n",
      "[4] Processing: samplenew3_00000151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000151\n",
      "\n",
      "[5] Processing: samplenew3_00000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000195\n",
      "\n",
      "[6] Processing: samplenew3_00000220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000220\n",
      "\n",
      "[7] Processing: samplenew3_00000261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000261\n",
      "\n",
      "[8] Processing: samplenew3_00000294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000294\n",
      "\n",
      "[9] Processing: samplenew3_00000340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000340\n",
      "\n",
      "[10] Processing: samplenew3_00000347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000347\n",
      "\n",
      "üìä Progress: 10 samples processed\n",
      "\n",
      "[11] Processing: samplenew3_00000360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000360\n",
      "\n",
      "[12] Processing: samplenew3_00000367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000367\n",
      "\n",
      "[13] Processing: samplenew3_00000378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000378\n",
      "\n",
      "[14] Processing: samplenew3_00000398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000398\n",
      "\n",
      "[15] Processing: samplenew3_00000425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000425\n",
      "\n",
      "[16] Processing: samplenew3_00000432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000432\n",
      "\n",
      "[17] Processing: samplenew3_00000447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000447\n",
      "\n",
      "[18] Processing: samplenew3_00000454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000454\n",
      "\n",
      "[19] Processing: samplenew3_00000455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000455\n",
      "\n",
      "[20] Processing: samplenew3_00000467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000467\n",
      "\n",
      "üìä Progress: 20 samples processed\n",
      "\n",
      "[21] Processing: samplenew3_00000482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000482\n",
      "\n",
      "[22] Processing: samplenew3_00000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000493\n",
      "\n",
      "[23] Processing: samplenew3_00000495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000495\n",
      "\n",
      "[24] Processing: samplenew3_00000496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000496\n",
      "\n",
      "[25] Processing: samplenew3_00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000500\n",
      "\n",
      "[26] Processing: samplenew3_00000501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000501\n",
      "\n",
      "[27] Processing: samplenew3_00000504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000504\n",
      "\n",
      "[28] Processing: samplenew3_00000511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000511\n",
      "\n",
      "[29] Processing: samplenew3_00000513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000513\n",
      "\n",
      "[30] Processing: samplenew3_00000535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000535\n",
      "\n",
      "üìä Progress: 30 samples processed\n",
      "\n",
      "[31] Processing: samplenew3_00000548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000548\n",
      "\n",
      "[32] Processing: samplenew3_00000579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000579\n",
      "\n",
      "[33] Processing: samplenew3_00000588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000588\n",
      "\n",
      "[34] Processing: samplenew3_00000593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000593\n",
      "\n",
      "[35] Processing: samplenew3_00000602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000602\n",
      "\n",
      "[36] Processing: samplenew3_00000605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000605\n",
      "\n",
      "[37] Processing: samplenew3_00000665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000665\n",
      "\n",
      "[38] Processing: samplenew3_00000668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000668\n",
      "\n",
      "[39] Processing: samplenew3_00000682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000682\n",
      "\n",
      "[40] Processing: samplenew3_00000723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000723\n",
      "\n",
      "üìä Progress: 40 samples processed\n",
      "\n",
      "[41] Processing: samplenew3_00000725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000725\n",
      "\n",
      "[42] Processing: samplenew3_00000744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000744\n",
      "\n",
      "[43] Processing: samplenew3_00000765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000765\n",
      "\n",
      "[44] Processing: samplenew3_00000803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000803\n",
      "\n",
      "[45] Processing: samplenew3_00000814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000814\n",
      "\n",
      "[46] Processing: samplenew3_00000821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000821\n",
      "\n",
      "[47] Processing: samplenew3_00000827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000827\n",
      "\n",
      "[48] Processing: samplenew3_00000829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000829\n",
      "\n",
      "[49] Processing: samplenew3_00000839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000839\n",
      "\n",
      "[50] Processing: samplenew3_00000891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: samplenew3_00000891\n",
      "\n",
      "üìä Progress: 50 samples processed\n",
      "\n",
      "üéâ Processing complete!\n",
      "‚úÖ Successfully processed: 50 samples\n",
      "‚ùå Errors encountered: 0 samples\n",
      "üíæ Results saved to: emotion_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize results storage\n",
    "results = {}\n",
    "processed_count = 0\n",
    "error_count = 0\n",
    "\n",
    "# Process each sample\n",
    "for enhanced_merged_by_name_key in list(enhanced_merged_by_name.keys()):\n",
    "    try:\n",
    "        sample_data = enhanced_merged_by_name[enhanced_merged_by_name_key]\n",
    "        \n",
    "        # Extract required fields\n",
    "        subtitle_text = sample_data.get('chinese_subtitle', '')\n",
    "        video_name = sample_data['name']\n",
    "        video_path = f'/home/paperspace/Downloads/25-07-11-affectgpt-dataset-mini100/video/{video_name}.mp4'\n",
    "        \n",
    "        # Skip if no subtitle\n",
    "        if not subtitle_text:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {video_name}: No Chinese subtitle\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{processed_count + 1}] Processing: {video_name}\")\n",
    "        \n",
    "        # Load and process video\n",
    "        pixel_values, num_patches_list = load_video(video_path, num_segments=10, max_num=1)\n",
    "        pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "        video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "        \n",
    "        # Emotional state analysis\n",
    "        question1 = video_prefix + '<subtitle>\\n' + subtitle_text + '\\n</subtitle>\\n' + 'Please identify the emotional states of the main character with specific reasons.'\n",
    "        response1, history = model.chat(tokenizer, pixel_values, question1, generation_config,\n",
    "                                       num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "        \n",
    "        # Emotion word extraction\n",
    "        question2 = \"\"\"Please extract emotion words into a list. Please separate different emotional categories with commas and output only the clearly identifiable emotional categories in a list format. If none are identified, please output an empty list. The output words have to be adjectives of feelings from those emotion words (internal state), not an action/expression (external manifestation).\n",
    "\n",
    "EXAMPLES:\n",
    "‚úì Good: [\"happy\", \"excited\", \"nervous\"]\n",
    "‚úó Bad: [\"smiling\", \"jumping\", \"talking loudly\"]\n",
    "\"\"\"\n",
    "        response2, history = model.chat(tokenizer, pixel_values, question2, generation_config, \n",
    "                                       history=history, return_history=True)\n",
    "        \n",
    "        # Store results\n",
    "        results[video_name] = {\n",
    "            'subtitle': subtitle_text,\n",
    "            'emotional_analysis': response1,\n",
    "            'emotion_words': response2,\n",
    "            'openset': sample_data.get('openset', ''),\n",
    "            'reason': sample_data.get('reason', ''),\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        processed_count += 1\n",
    "        print(f\"‚úÖ Completed: {video_name}\")\n",
    "        \n",
    "        # Print progress every 10 samples\n",
    "        if processed_count % 10 == 0:\n",
    "            print(f\"\\nüìä Progress: {processed_count} samples processed\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {video_path}\")\n",
    "        error_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {video_name}: {str(e)}\")\n",
    "        error_count += 1\n",
    "\n",
    "# Save results to file\n",
    "with open('emotion_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nüéâ Processing complete!\")\n",
    "print(f\"‚úÖ Successfully processed: {processed_count} samples\")\n",
    "print(f\"‚ùå Errors encountered: {error_count} samples\")\n",
    "print(f\"üíæ Results saved to: emotion_analysis_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bf07d-d374-49bf-98bc-e0a72bcf6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# pure-text conversation (Á∫ØÊñáÊú¨ÂØπËØù)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Can you tell me a story?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image single-round conversation (ÂçïÂõæÂçïËΩÆÂØπËØù)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image multi-round conversation (ÂçïÂõæÂ§öËΩÆÂØπËØù)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, combined images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÊãºÊé•ÂõæÂÉè)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "question = '<image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, separate images (Â§öÂõæÂ§öËΩÆÂØπËØùÔºåÁã¨Á´ãÂõæÂÉè)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# batch inference, single image per sample (ÂçïÂõæÊâπÂ§ÑÁêÜ)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# video multi-round conversation (ËßÜÈ¢ëÂ§öËΩÆÂØπËØù)\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n",
    "video_path = './examples/red-panda.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + 'What is the red panda doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Describe this video in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "25-07-11-internvl-2",
   "language": "python",
   "name": "25-07-11-internvl-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
